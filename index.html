<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <!-- Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-62DE15HYK6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-62DE15HYK6');
  </script>

  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="F3: Fast Feature Field - A Predictive Representation of Events.">
  <meta property="og:title" content="F3: Fast Feature Field"/>
  <meta property="og:description" content="F3: Fast Feature Field - A Predictive Representation of Events"/>
  <meta property="og:url" content="https://seas.upenn.edu/~richeek/f3"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="event cameras, neuromorphic vision, depth estimation, optical flow, semantic segmentation, robotics, computer vision, machine learning, hash encoding, deep sets">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Fast Feature Field (F3)</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Fast Feature Field (F<sup>3</sup>): A Predictive Representation of Events</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="http://seas.upenn.edu/~richeek/" target="_blank">Richeek Das</a>,</span>
                <span class="author-block">
                  <a href="https://www.cis.upenn.edu/~kostas/" target="_blank">Kostas Daniilidis</a>,</span>
                  <span class="author-block">
                    <a href="https://pratikac.github.io/" target="_blank">Pratik Chaudhari</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">GRASP Laboratory, University of Pennsylvania</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2509.25146.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                      </a>
                    </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2509.25146" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/grasp-lyrl/fast-feature-fields" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>

                  <!-- Coming Soon Video Link -->
                  <span class="link-block">
                    <a href="#" class="external-link button is-normal is-rounded is-dark" style="pointer-events: none; opacity: 0.6;">
                      <span class="icon">
                        <i class="fas fa-video"></i>
                      </span>
                      <span>Video (<span style="color: red;">Coming Soon!</span>)</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper develops a mathematical argument and algorithms for building representations of data from event-based cameras, that we call Fast Feature Field (F<sup>3</sup>). We learn this representation by predicting future events from past events and show that it preserves scene structure and motion information. F<sup>3</sup> exploits the sparsity of event data and is robust to noise and variations in event rates. It can be computed efficiently using ideas from multi-resolution hash encoding and deep setsâ€”achieving 120 Hz at HD and 440 Hz at VGA resolutions. F<sup>3</sup> represents events within a contiguous spatiotemporal volume as a multi-channel image, enabling a range of downstream tasks. We obtain state-of-the-art performance on optical flow estimation, semantic segmentation, and monocular metric depth estimation, on data from three robotic platforms (a car, a quadruped robot and a flying platform), across different lighting conditions (daytime, nighttime), environments (indoors, outdoors, urban, as well as off-road) and dynamic vision sensors (resolutions and event rates). Our implementations can predict these tasks at 25-75 Hz at HD resolution.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Figure 1 -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
        <img src="static/images/figure1/figure1.webp" alt="F3 Overview Figure">
    </div>
  </div>
</section> -->
<!-- End Figure 1 -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">F<sup>3</sup> can be used for many different tasks</h2>
        <p style="margin-bottom: 2rem;"></pstyle>It is possible to use F<sup>3</sup> in any standard computer vision algorithm or neural architecture built for RGB data. F<sup>3</sup> can be the foundation for a variety of robotic perception tasks. We show state-of-the-art performance on data from three platforms (driving, quadruped locomotion and a flying platform) and four tasks (supervised semantic segmentation, unsupervised optical flow, unsupervised stereo matching, and supervised monocular metric depth estimation). These sequences showcase F<sup>3</sup> performing multiple perception tasks simultaneouslyâ€”demonstrating its versatility as a foundation for robotic perception.</p>
      </div>
      
      <div class="container is-max-desktop">
        <video poster="" id="overview_combined" autoplay muted loop height="100%">
          <source src="static/videos/overview_combined.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          <strong>Overview:</strong> Driving scenes in Philadelphia demonstrating multi-task capabilities in urban environments
        </h2>
      </div>

      <div class="container" style="margin-top: 2rem;">
        <p><strong>F<sup>3</sup> manages to realize the benefits of event cameras in low-latency and low-light scenarios, especially where standard RGB cameras struggle. </strong> Moving objects like pedestrians and vehicles are captured reliably in events even under challenging lighting conditions. See examples below:</p>
      </div>

      <!-- Highlighted Scenarios -->
      <div class="container" style="margin-top: 2rem;">
        <div class="scenario-nav">
          <button class="scenario-btn active" onclick="showScenario('pedestrians-highlighted')" style="font-size: 1.5em; padding: 10px 20px;">
            F<sup>3</sup> spots pedestrians which RGB can't
          </button>
          <button class="scenario-btn" onclick="showScenario('structure-motion-highlighted')" style="font-size: 1.5em; padding: 10px 20px;">
            F<sup>3</sup> captures fine structure and motion
          </button>
        </div>
        <div class="scenario-carousel-container">
          <!-- Pedestrians Scenario -->
          <div id="scenario-pedestrians-highlighted" class="scenario-content active">
            <div class="carousel results-carousel">
              <div class="item">
                <video poster="" id="night2" autoplay controls muted loop height="100%">
                  <source src="static/videos/night2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Nighttime:</strong> Extreme low-light conditions
                </h2>
              </div>
              
              <div class="item">
                <video poster="" id="ucity1" autoplay controls muted loop height="100%">
                  <source src="static/videos/ucity1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Urban Scene:</strong> University City
                </h2>
              </div>
              <div class="item">
                <video poster="" id="rittenhouse3" autoplay controls muted loop height="100%">
                  <source src="static/videos/rittenhouse3.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Urban Scene:</strong> Rittenhouse Square - High dynamic range
                </h2>
              </div>
            </div>
          </div>
          <!-- Structure and Motion Scenario -->
          <div id="scenario-structure-motion-highlighted" class="scenario-content">
            <div class="carousel results-carousel">
              <div class="item">
                <video poster="" id="rittenhouse2" autoplay controls muted loop height="100%">
                  <source src="static/videos/rittenhouse2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Urban Scene:</strong> Rittenhouse Square - Dense pedestrian traffic
                </h2>
              </div>
              
              <div class="item">
                <video poster="" id="cityhall2" autoplay controls muted loop height="100%">
                  <source src="static/videos/cityhall2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Urban Scene:</strong> City Hall - Complex urban intersection
                </h2>
              </div>
              
              <div class="item">
                <video poster="" id="highway1" autoplay controls muted loop height="100%">
                  <source src="static/videos/highway1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered" style="font-size: 1.5em;">
                  <strong>Highway Scene:</strong> High-speed driving scenario
                </h2>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- Architecture subsection -->
      <div class="container" style="margin-top: 3rem;">
        <h2 class="title is-3">F<sup>3</sup> architecture is designed specifically for events</h2>
        <div class="content">
          <p>
            F<sup>3</sup> is a predictive representation of events. It is a statistic of past events sufficient to predict future events. We prove that such a representation retains information about the structure and motion in the scene. F<sup>3</sup> achieves low-latency computation by exploiting the sparsity of event data using a multi-resolution hash encoder and permutation-invariant architecture.  Our implementation can compute F<sup>3</sup> at 120 Hz and 440 Hz at HD and VGA resolutions, respectively, and can predict different downstream tasks at 25-75 Hz at HD resolution. These HD inference rates are roughly 2-5 times faster than the current state-of-the-art event-based methods. Please refer to the <a href="https://arxiv.org/abs/2509.25146" target="_blank"><strong>paper</strong></a> for more details.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-three-quarters">
            <img src="static/images/architecture/arch.webp" alt="F3 Overview Diagram">
            <p class="has-text-centered" style="margin-top: 1rem; font-style: italic;">
              An overview of the neural architecture for Fast Feature Field (F<sup>3</sup>).
            </p>
          </div>
        </div>
      </div>

      <!-- Specific scenarios subsection -->
      <div class="container" style="margin-top: 3rem;">
        <h2 class="title is-3">F<sup>3</sup> is robust</h2>
        <p style="margin-bottom: 2rem;">F<sup>3</sup>-based approaches work robustly without additional training across data from different robotic platforms, lighting and environmental conditions (daytime vs. night-time, indoors vs. outdoors, urban vs. off-road) and dynamic vision sensors (with different resolutions and event rates). <strong>See examples of F<sup>3</sup> generalizing to different</strong></p> 
        
        <!-- Scenario Navigation Buttons -->
        <div class="scenario-nav">
          <button class="scenario-btn active" onclick="showScenario('lighting')" style="font-size: 1.1em">
            Lighting conditions
          </button>
          <button class="scenario-btn" onclick="showScenario('environments')" style="font-size: 1.1em">
            Environments (City to Forest)
          </button>
          <button class="scenario-btn" onclick="showScenario('robots')" style="font-size: 1.1em">
            Robot platforms (Car to Quadruped/Drone)
          </button>
          <button class="scenario-btn" onclick="showScenario('sensors')" style="font-size: 1.1em">
            Event vision sensors (HD to VGA)
          </button>
        </div>

        <!-- Single Carousel Container that changes content -->
        <div class="scenario-carousel-container">
          
          <!-- Lighting Scenario -->
          <div id="scenario-lighting" class="scenario-content active">
            <div class="carousel results-carousel">
              <div class="item">
                <video poster="" id="night1" autoplay controls muted loop height="100%">
                  <source src="static/videos/night1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Nighttime:</strong> Low-light urban driving (demonstrates event camera advantages)
                </h2>
              </div>
<!-- 
              <div class="item">
                <video poster="" id="night2" autoplay controls muted loop height="100%">
                  <source src="static/videos/night2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Nighttime:</strong> Extreme low-light conditions
                </h2>
              </div> -->

              <div class="item">
                <video poster="" id="tunnel1" autoplay controls muted loop height="100%">
                  <source src="static/videos/tunnel1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Tunnel:</strong> Rapid lighting transitions (high dynamic range)
                </h2>
              </div>
            </div>
          </div>

          <!-- Environments Scenario -->
          <div id="scenario-environments" class="scenario-content">
            <div class="carousel results-carousel">
              <!-- <div class="item">
                <video poster="" id="forest1" autoplay controls muted loop height="100%">
                  <source src="static/videos/forest1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Off-Road:</strong> Forest environment (trained on urban, generalizes to off-road)
                </h2>
              </div> -->

              <div class="item">
                <video poster="" id="forest2" autoplay controls muted loop height="100%">
                  <source src="static/videos/forest2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Off-Road:</strong> Dense forest with complex motion patterns
                </h2>
              </div>

              <div class="item">
                <video poster="" id="forest3" autoplay controls muted loop height="100%">
                  <source src="static/videos/forest3.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Off-Road:</strong> Forest scene with challenging structure extraction
                </h2>
              </div>
            </div>
          </div>

          <!-- Robots Scenario -->
          <div id="scenario-robots" class="scenario-content">
            <div class="carousel results-carousel">
              <div class="item">
                <video poster="" id="spot1" autoplay controls muted loop height="100%">
                  <source src="static/videos/spot1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Platform:</strong> Spot Quadruped Robot (cross-platform generalization)
                </h2>
              </div>

              <div class="item">
                <video poster="" id="spot2" autoplay controls muted loop height="100%">
                  <source src="static/videos/spot2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Platform:</strong> Spot - Indoor navigation
                </h2>
              </div>

              <div class="item">
                <video poster="" id="spot3" autoplay controls muted loop height="100%">
                  <source src="static/videos/spot3.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Platform:</strong> Spot - Complex indoor environment
                </h2>
              </div>

              <div class="item">
                <video poster="" id="falcon1" autoplay controls muted loop height="100%">
                  <source src="static/videos/falcon1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Platform:</strong> Falcon Flying Platform (aerial perspective)
                </h2>
              </div>

              <div class="item">
                <video poster="" id="falcon2" autoplay controls muted loop height="100%">
                  <source src="static/videos/falcon2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>Platform:</strong> Falcon - Rapid motion and altitude changes
                </h2>
              </div>
            </div>
          </div>

          <!-- Sensors Scenario -->
          <div id="scenario-sensors" class="scenario-content">
            <div class="carousel results-carousel">
              <div class="item">
                <video poster="" id="dsec1" autoplay controls muted loop height="100%">
                  <source src="static/videos/dsec1.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>DSEC Dataset:</strong> Benchmark scenario 1
                </h2>
              </div>

              <div class="item">
                <video poster="" id="dsec2" autoplay controls muted loop height="100%">
                  <source src="static/videos/dsec2.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>DSEC Dataset:</strong> Benchmark scenario 2
                </h2>
              </div>

              <div class="item">
                <video poster="" id="dsec3" autoplay controls muted loop height="100%">
                  <source src="static/videos/dsec3.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>DSEC Dataset:</strong> Benchmark scenario 3
                </h2>
              </div>

              <div class="item">
                <video poster="" id="dsec4" autoplay controls muted loop height="100%">
                  <source src="static/videos/dsec4.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  <strong>DSEC Dataset:</strong> Benchmark scenario 4
                </h2>
              </div>
            </div>
          </div>

        </div>
      </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">F<sup>3</sup> can be used for many different tasks</h2>
      <p>It possible to use F<sup>3</sup>  in any standard computer vision algorithm or neural architecture built for RGB data. F<sup>3</sup> can be the foundation for a variety of robotic perception tasks. We show state-of-the-art performance on data from three platforms (driving, quadruped locomotion and a flying platform) and four tasks (supervised semantic segmentation, unsupervised optical flow, unsupervised stereo matching, and supervised monocular metric depth estimation).</p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/depth/monocular_depth.webp" alt="Monocular Depth Estimation"/>
        <h2 class="subtitle has-text-centered">
          Monocular Depth Estimation.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/seg/segmentation.webp" alt="Semamtic Segmentation"/>
        <h2 class="subtitle has-text-centered">
          Semantic Segmentation.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/flow/flow.webp" alt="Optical Flow Estimation"/>
        <h2 class="subtitle has-text-centered">
         Unsupervised Optical Flow Estimation.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/stereo/stereo_depth.webp" alt="Stereo Disparity with Events"/>
      <h2 class="subtitle has-text-centered">
        Stereo Disparity from stereo events without any learning!
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">F<sup>3</sup> is robust</h2>
      <p>F<sup>3</sup>-based approaches work robustly without additional training across data from different robotic platforms, lighting and environmental conditions (daytime vs. night-time, indoors vs. outdoors, urban vs. off-road) and dynamic vision sensors (with different resolutions and event rates).</p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/robustness/spot_falcon.webp" alt="Robustness to Robotic Platforms"/>
        <h2 class="subtitle has-text-centered">
          Different robotic platforms: trained on a car, works on a quadruped robot (Spot) and a flying platform (Falcon).
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/robustness/car_nighttime.webp" alt="Robustness to Lighting"/>
        <h2 class="subtitle has-text-centered">
          Extreme changes in lighting: trained on daytime, works at nighttime.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/robustness/car_m3ed2dsec.webp" alt="Robustness to sensor in use"/>
        <h2 class="subtitle has-text-centered">
          Different event cameras: trained on an HD event vision sensor, works on a VGA sensor.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/robustness/car_forest.webp" alt="Robustness to environments"/>
      <h2 class="subtitle has-text-centered">
        Highly differing environments: trained on urban driving sequences, works off-road in a forest. <br>
        <span style="font-size: 1rem;">(Like the scene in the first image row, it is often difficult to discern the branches of the closer tree from the background using RGB images, <br> but the motion information in events makes it doable. F<sup>3</sup> helps us extract this kind of structural information from events much more easily.)</span>
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<hr>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{das2025fastfeaturefield,
  title={Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events}, 
  author={Richeek Das and Kostas Daniilidis and Pratik Chaudhari},
  year={2025},
  eprint={2509.25146},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2509.25146},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
